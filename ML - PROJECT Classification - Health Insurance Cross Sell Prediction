{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1715785775317}],"collapsed_sections":["w6K7xa23Elo4","Yfr_Vlr8HBkt","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","yiiVWRdJDDil","T5CmagL3EC8N","bmKjuQ-FpsJ3","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Project Name    - Health Insurance Cross Sell Prediction**\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### Name - Azmat\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["This project is divided into multiple sections and each section will have its own importance to our problem statement. the approach that we will be following in this project is given as-\n","\n","Section 1: Data overview: In this section, we will be importing important libraries and simply loading our dataset into google collab and will explore the basic information about the data.\n","\n","Section 2: Exploratory Data Analysis: In this section, We will focus on Exploratory data analysis of the dataset using various methods and visualization plots and will be extracting the information from this dataset as much as we can.\n","\n","Section 3: Outliers handling: In this section, we will be dealing with outliers in our dataset and will see how to define our outlier criteria and deal with outliers.\n","\n","Section 4: Feature Engineering: In this Section, we will be splitting the data into training and testing datasets then we will be scaling the data. Section 6: Model implementation: In this section, we will be applying and evaluating the ML models to predict the results.\n","\n","Section 7: Model Explainability: In this section, we will use model explainability to explain the results.\n","\n","Section 8: In this section, we will be giving a quick summary of the entire notebook."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n","\n","An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n","\n","For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n","\n","Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.\n","\n","Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n","\n","Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np\n","import pandas as pd\n","\n","# Importing  tools for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importing libraries for hypothesis testing\n","from scipy.stats import uniform\n","from scipy.stats import norm\n","from scipy.stats import chi2\n","from scipy.stats import t\n","from scipy.stats import f\n","\n","# Importing libraries for data pre-processing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","from imblearn.over_sampling import SMOTE\n","\n","# Importing Machine Learning algorithm libraries\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","\n","# Importing Classification algorithm metrics\n","from sklearn import metrics\n","from mlxtend.plotting import plot_confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import classification_report\n"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df = pd.read_csv('/content/drive/MyDrive/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"],"metadata":{"id":"or8GduwB5EE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","data_df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.tail()"],"metadata":{"id":"_VhiHp0Q5h87"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","rows = data_df.shape[0]\n","columns = data_df.shape[1]\n","print(f'Number of rows: {rows}')\n","print(f'Number of columns: {columns}')\n"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","data_df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","data_df[data_df.duplicated()]"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","\n","missing_count = data_df.isnull().sum()\n","missing_count"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize = (10,5))\n","sns.heatmap(data_df.isna().transpose(), cmap = 'coolwarm' )\n","plt.title('Missing Values')\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["The dataset consists of 381,109 customer responses regarding vehicle insurance. The dependent variable, \"Response,\" indicates whether customers purchased the vehicle insurance subscription, with 0 representing non-purchase and 1 representing purchase. The dataset contains 11 independent variables, including integer, float, and string data types. Notably, the dataset is free from null values, missing values, and duplicates across its columns."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","data_df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","data_df.describe()"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["id: Unique customer identifier.\n","\n","Gender: Customer's gender.\n","\n","Age: Customer's age.\n","\n","Driving License: Indicates if the customer has a driving license (0: No, 1: Yes).\n","\n","Region Code: Unique code representing the customer's region.\n","\n","Previously Insured: Indicates if the customer already has vehicle insurance (0: No, 1: Yes).\n","\n","Vehicle Age: Age of the customer's vehicle.\n","\n","Vehicle Damage: Indicates if the customer's vehicle has been previously damaged (0: No, 1: Yes).\n","\n","Annual Premium: Yearly premium amount paid by the customer.\n","\n","Policy Sales Channel: Anonymized code representing the channel used to reach out to the customer.\n","\n","Vintage: Number of days the customer has been associated with the company.\n","\n","Response: Customer's interest in vehicle insurance (0: Not interested, 1: Interested)."],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","data_df.nunique()"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Categorizing Age feature\n","data_df['Age_Group'] = data_df['Age'].apply(lambda x:'YoungAge' if x >= 20 and x<=45 else 'MiddleAge' if x>45 and x<=65 else 'OldAge')\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Splitting the premium based on mean\n","data_df['Low_premium'] = data_df['Annual_Premium']<= data_df['Annual_Premium'].mean()\n","data_df['High_premium'] = data_df['Annual_Premium']> data_df['Annual_Premium'].mean()"],"metadata":{"id":"8eaGaR-z111z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.head()"],"metadata":{"id":"3a0TWGDM2GgS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["I categorized the age of the vehicle for the efficient analization of the data\n","\n","And also categorized premium into two parts one is Low_premium and the second one is High_premium"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"markdown","source":["**Let's check interest of people towards buying vehicle insurance**"],"metadata":{"id":"RBmsEsLi8CvF"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","\n","# Count plot of dependent variable\n","\n","sns.countplot(x=data_df['Response'])\n","plt.title('Response of people towards vehichle insurance', size =16)\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["Answer - I chose this because a Seaborn countplot is ideal for visually highlighting the differences between two values of a variable."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Answer - Most people choose not to buy vehicle insurance."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Answer - The plot reveals a small customer base that prefers purchasing vehicle insurance, indicating a negative impact on the business."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"markdown","source":["**Let's check which gender was more interested in subscribing to Vehicle Insurance.**"],"metadata":{"id":"L_Jsceu09WWs"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Grouping the gender with response\n","\n","data_df_gender = data_df.groupby('Gender').sum()['Response']\n","# Plotting\n","data_df_gender.plot(kind = 'bar',subplots= True ,figsize=(10,5))\n","plt.title('Gender response towards vehichle insurance',size = 16)"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Answer - I chose this because a bar chart from the matplotlib library is a useful way to visually represent the proportions of different values within a variable."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Answer - The bar chart indicates that there was a greater level of interest among males in subscribing to Vehicle Insurance"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Answer - The data clearly indicates that a significant majority of customers are males, which aligns with the commonly observed trend in society."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"markdown","source":["**Let's check customers of which age are in majority**"],"metadata":{"id":"rUuk-odT-i6l"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","\n","#visualization on the basis of Age column\n","sns.set(rc={'figure.figsize':(12,6)})\n","sns.distplot(data_df['Age'])"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Answer - I chose this chart because the distplot, or distribution plot, visually represents the spread and pattern of continuous data variables. It provides an overview of the data distribution using Seaborn's distplot function."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Answer - Based on the plot, it is evident that the age range of 22-25 has the highest number of customers."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["\n","Answer - The majority of responses received are from the age groups 22-25, suggesting that targeting these age groups can contribute to business growth."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"markdown","source":["**Let's check people of which age group are more intrested**"],"metadata":{"id":"RK0gbfV3_IQw"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","\n","# Grouping age column with response column\n","data_df_group = data_df.groupby('Age_Group').sum()['Response']\n","\n","# plotting the result\n","data_df_group.plot(kind = 'line' , subplots = True, figsize = (10,5))"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["Answer - I chose Line plot because it is useful for showing how a variable changes in relation to another variable."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Answer - The analysis reveals a notable inclination among young individuals towards vehicle insurance, while comparatively lower interest is observed among older age groups."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["Answer - The higher interest in vehicle insurance from the young age group, combined with their larger population representation in a country, is a positive outcome."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"markdown","source":["**Let's check how many customers are high premium subscribers**"],"metadata":{"id":"V-5r5SXN_6rL"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","\n","# count of high premium subscribers\n","sns.set(rc={'figure.figsize':(10,6)})\n","sns.countplot(x=data_df['High_premium'])\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Answer - I chose this because a Seaborn countplot is ideal for visually highlighting the differences between two values of a variable."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["Answer - Based on the plot, it is clear that individuals tend to prefer higher-end premiums over lower-end options."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Answer - The plot indicates that customers who selected vehicle insurance are more inclined towards choosing expensive premium options. This is a positive aspect for the business as it contributes to increased revenue generation."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"markdown","source":["**Let's count age wise subscription of vehicle insurance**"],"metadata":{"id":"LaGZPa4_AkBY"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","\n","# Age wise plotting of graph\n","data_df_age = data_df.groupby('Age').sum()['Response']\n","#plotting the result\n","data_df_age.plot(kind='bar', subplots = True , figsize=(20,10))\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["Answer - I chose this because a bar chart from the matplotlib library is a useful way to visually represent the proportions of different values within a variable."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Answer - The plot clearly shows that there is a significant level of interest and response regarding Vehicle Insurance within the age range of 40 to 48."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Answer - We can observe that there is a considerable number of positive responses from the age groups above 40 to 48. Furthermore, it is noticeable that there is a consistent upward trend starting from the age of 28."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"markdown","source":["**Let's check non subscribers (age - wise)**"],"metadata":{"id":"pNAhUyGfBfa1"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","\n","# code to find non-subscribers (age wise)\n","data_df_age = data_df[data_df['Response']==0]['Age'].value_counts().sort_index()\n","#plotting the result\n","data_df_age.plot(kind = 'bar', subplots = True , figsize=(20,10))\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["Answer - I chose this because a bar chart from the matplotlib library is a useful way to visually represent the proportions of different values within a variable."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["Answer - The plot clearly shows that there is a noticeable number of negative responses regarding Vehicle Insurance within the age groups of 21 to 25."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["Answer - It appears that the age groups 21 to 25 have shown less interest in vehicle insurance. This could be due to reasons such as not having a driver's license or lacking awareness about the importance of insurance. It's important to note that this is not necessarily a negative impact, as they are still potential customers who may consider vehicle insurance in the future"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"markdown","source":["**Let's check number of responses by people who are previously insured and not insured**"],"metadata":{"id":"pR3EuN2KCSGs"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","\n","# value count of previously insured\n","data_df['Previously_Insured'].value_counts()"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#Grouping positive responses with previously insured customers\n","data_df_pre_insured = data_df[data_df['Response']==1].groupby('Previously_Insured').count()['Response']\n","data_df_pre_insured"],"metadata":{"id":"Bqme0uB0CjVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the pie chart\n","data_df_pre_insured.plot(kind = 'pie', subplots = True , figsize=(7,7))"],"metadata":{"id":"NWHh90tnCs1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["Answer - I chose pie chart, which is created using the Matplotlib library, is a useful tool for visually representing the proportion or distribution of different values within a variable."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Answer - The Pie Chart illustrates the distribution of responses among individuals based on their previous insurance status. It indicates that people who do not have prior insurance coverage tend to overwhelmingly choose to opt in for vehicle insurance."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["\n","Answer - By observing the Pie Chart, the company can identify an opportunity to specifically target customers who do not have previous vehicle insurance. These customers are more likely to be open to considering and opting in for vehicle insurance. Therefore, the company can focus its efforts on actively reaching out to this particular group of customers."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"markdown","source":["**Let's check the customer's subscription prices**"],"metadata":{"id":"c7vGsvf9DLh7"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","\n","# code for distribution of annual premium\n","sns.distplot(data_df[\"Annual_Premium\"])\n","plt.figure(figsize = (12,6))\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["Answer - I chose a distribution plot beacause it is a visual representation that shows how data is distributed or spread out. It is particularly useful for continuous data variables."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Answer - The majority of customers tend to have subscription prices that are centered around $50,000."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["Answer - The plot indicates that customers who chose vehicle insurance are more inclined towards higher premium options. This is advantageous for the business as it leads to increased revenue."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"markdown","source":["**Let's check response from customers**"],"metadata":{"id":"VmJTBtVtDxAj"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","\n","\n","data_df['Vintage'].value_counts()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_vintage = data_df[data_df['Response']==1].groupby('Vintage').count()['Response']\n","# Violin plot for vintage\n","sns.violinplot(y=data_df_vintage)"],"metadata":{"id":"H1Bq-3SVD-Ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["Answer - Violin plots are a type of visual representation similar to box plots. However, they provide additional information by displaying the probability density of the data at different values."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["Answer - The violin plot indicates that there is a higher frequency of positive responses from customers who have been loyal for 150 or more days. However, the positive response rate decreases after the 180-day mark, suggesting a decline in customer satisfaction or engagement beyond this period."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["Answer - To enhance customer loyalty and engagement, the company can strategically extend the tenure of a customer by providing special offers or incentives once they have reached an average of 180 days. By recognizing this milestone, the company can encourage customers to continue their association and foster long-term relationships."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"markdown","source":["Let's check the positive responses from the customers across all the regions"],"metadata":{"id":"q0sZBB30EkyR"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","\n","# grouping\n","data_df_region = data_df.groupby('Region_Code').sum()['Response']\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the response\n","data_df_region.plot(kind='line', subplots = True , figsize = (14,5) )"],"metadata":{"id":"GYCHTwgnE0z3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["Answer - I chose Line plot because it is useful for showing how a variable changes in relation to another variable."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["Answer - This line graph represents the favorable feedback received from customers in various regions. The data shows that Area code 28 received the highest number of positive responses, followed by codes 8 and 46."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["Answer - The company should pay close attention to area codes 28, 8, and 46 as these regions hold great potential for selling vehicle insurance."],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"markdown","source":["**Histogram of all the features**"],"metadata":{"id":"R-i-Ja_DFRCE"}},{"cell_type":"code","source":["# Chart - 12 visualization code\n","\n","# Code for histogram\n","data_df.hist(bins = 20 , figsize = (18,12))"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["Answer - A histogram, created using the matplotlib library, is a useful tool for visualizing the distribution of values for a specific variable."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["Answer - This provides a visual representation of how the values are spread across all the columns in the dataset."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Answer - This shows how the values are divided or spread out within each column of the dataset."],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"code","source":["data_df.head()"],"metadata":{"id":"BxP-1XyvGBbu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# creating copy to keep original data safe\n","data_df_new = data_df.copy()"],"metadata":{"id":"HLMlsUFoGP33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# droping the low premium cloumn\n","data_df_new.drop(columns = (['Low_premium']), inplace= True)"],"metadata":{"id":"jQVn29XhGUlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting the 'High_premium' column to string data type\n","data_df_new['High_premium'] = data_df_new['High_premium'].astype(str)"],"metadata":{"id":"55A_NiCPGdWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Perform one-hot encoding on categorical columns using pandas get_dummies function\n","data_df_new = pd.get_dummies(data_df_new,drop_first= False)"],"metadata":{"id":"E3zdxrT0GmwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","data_df_new.info()"],"metadata":{"id":"2jRMpID8Gwk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"markdown","source":["**Customers who own Driving License or not proportion**"],"metadata":{"id":"cM5G6Tb0GHiO"}},{"cell_type":"code","source":["# Chart - 13 visualization code\n","\n","sns.countplot(x='Driving_License', data=data_df)\n","\n","# setting chart title\n","plt.title('Customers who own Driving License or not proportion')\n","\n","# display chart\n","plt.show()\n","\n","# printing the counts for reference\n","print(data_df.Driving_License.value_counts())\n","\n"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["Count plot is well suited for finding the counts and plotting the count values."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["There are 380297 people who own Driving License and 812 don't"],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["Its found that most of the people who own driving license owns a car"],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","\n","# Creating a figure with a specific size\n","plt.figure(figsize=(16, 8))\n","\n","# Compute the correlation matrix\n","correlation = data_df_new.corr()\n","\n","# Creating a heatmap\n","sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')\n","\n"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculatig the multicollinearity of all columns\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","def calc_vif(X):\n","\n","    # Calculating VIF\n","    vif = pd.DataFrame()\n","    vif[\"variables\"] = X.columns\n","    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","    return(vif)"],"metadata":{"id":"LcR-DsPUgzSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"QBcV-FM8g9GK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['Gender_Female']) , inplace =True)"],"metadata":{"id":"P41t8sBF9pKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"fDkRZ5uK9u7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['Vehicle_Age_> 2 Years']) , inplace =True)"],"metadata":{"id":"8MhfMBLQ97Ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"l_UDeb5k-DaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"HekOn3Hp-axI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['Age_Group_YoungAge']) , inplace =True)"],"metadata":{"id":"fCO1kzhB-n_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"U99WcRrC-wQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['High_premium_True']) , inplace =True)"],"metadata":{"id":"MOfTsXI0-3QD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"1qX_xzZl--aD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['Driving_License']) , inplace =True)"],"metadata":{"id":"h0m6SaIm_Ixw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"4VKHldzH_V_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df_new.drop(columns = (['Age']) , inplace =True)"],"metadata":{"id":"LTdiXq43_ooS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(data_df_new[[i for i in data_df_new.describe().columns if i not in ['Response']]])"],"metadata":{"id":"mdj-dgP3_tAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Correlation\n","plt.figure(figsize=(15,8))\n","correlation = data_df_new.corr()\n","sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"],"metadata":{"id":"ExnHskcr_6Bh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["Answer - I chose this beacuse a correlation plot visually represents the relationships between variables in a dataset. It displays the correlation of each variable with itself and with other columns using a heatmap of colors."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["Answer - After dropping the columns whose VIF scores are higher than 10, we plotted the correlation heatmap plot to visualize ideal multicollenearity among the columns."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","sns.pairplot(data_df_new)"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["Answer - The Seaborn Pairplot is a useful tool for visualizing relationships between variables in a dataset. This makes it easier to interpret and understand the data, as it condenses a large amount of information into a single figure."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["Answer - By generating scatterplots the Pairplot function in Seaborn allowed us to visually explore and understand the relationships between different columns in the dataset"],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Answer - The average annual premium for a vehicle insurance is greater than 20,000.\n","\n","Null hypothesis H0: Average Annual premium not > 20,000.\n","\n","Alternate hypothesis Ha: Average Annual premium > 20,000."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","\n","# Perform Statistical Test to obtain P-Value\n","\n","annual_premium_sample = data_df['Annual_Premium'].sample(500)\n","annual_premium_mean = np.mean(annual_premium_sample)\n","annual_premium_std = np.std(annual_premium_sample)"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing test statistic\n","\n","ts = (annual_premium_mean-20000)/(annual_premium_std/(np.sqrt(500)))\n","print(ts)\n",""],"metadata":{"id":"uqq_es8tGEgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability\n","\n","prob_z = norm.cdf(14.62, 0, 1)\n","print(prob_z)\n"],"metadata":{"id":"E9Wei_neGM81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# P-Value\n","p1 = 1-prob_z\n","print(p1)"],"metadata":{"id":"9M9krum8GSPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The p value is smaller than significance level. So, we will reject the null hypothesis and accept the alternative hypothesis.**"],"metadata":{"id":"vA9eERZPhUoO"}},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["Answer - We have chosen Z-test to obtain p-value."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["Answer -As we are performing hypothesis testing for mean, we have chosen Z-test to obtain p-value. The probability we have obtained is close to 100%, so we have sufficient evidence to reject H0. Therefore, the average anuual premium is greater than 20,000."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["HYPOTHESIS : As the number of days a customer is associated with the company increase, the chances that the customer will opt in for vehicle insurance increases."],"metadata":{"id":"iPeWk4Lthz1M"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["The average age of the customer is greater than 30.\n","\n","Null hypothesis H0: Average age not > 30.\n","\n","Alternate hypothesis Ha: Average age > 30."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Perform Statistical Test to obtain P-Value\n","\n","age_sample = data_df['Age'].sample(500)\n","age_mean = np.mean(age_sample)\n","age_std = np.std(age_sample)"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Computing test statistic\n","\n","ts = (age_mean-30)/(age_std/(np.sqrt(500)))\n","print(ts)"],"metadata":{"id":"jaAdyNUKHeGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Calculating the probability\n","\n","prob_z = norm.cdf(13.48, 0, 1)\n","print(prob_z)\n","\n","# P-Value\n","p1 = 1-prob_z\n","print(p1)"],"metadata":{"id":"6ty0oMfMHjnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"sp62geCamchN"}},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["Answer - We have chosen Z-test to obtain p-value."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["Answer - As we are performing hypothesis testing for mean, we have chosen Z-test to obtain p-value. The probability we have obtained is close to 100%, so we have sufficient evidence to reject H0. Therefore, the average age of customer is greater than 30."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["The Standard deviation of annual premium is 10,000.\n","\n","Null hypothesis H0: Standard deviation != 10,000.\n","\n","Alternate hypothesis Ha: Standard deviation = 10,000."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Perform Statistical Test to obtain P-Value\n","\n","ap_sample = data_df['Annual_Premium'].sample(50)\n","S2 = (np.std(ap_sample))**2"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing test statistic\n","\n","ts3 = (49 * S2)/(10000*10000)\n","print(ts3)"],"metadata":{"id":"NNrSrMSR3f8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability\n","prob = chi2.cdf(158.82,49)\n","print(prob)"],"metadata":{"id":"tw3W3OiB3mz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["We have chosen Chi2-test to obtain p-value."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["As we are performing hypothesis testing for standard deviation, we have chosen Chi2-test to obtain p-value. The probability we have obtained is 99.99%, so we have sufficient evidence to reject H0. Therefore, the standard deviation of annual premium is 10,000."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["Answer - Since our dataset does not contain any missing values, there is no requirement to address or handle them."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["\n","# Handling Outliers & Outlier treatments\n","# Plotting the boxplot for 'Annual_Premium'\n","\n","sns.boxplot(x=data_df['Annual_Premium'])\n","\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding the IQR\n","\n","percentile25 = data_df['Annual_Premium'].quantile(0.25)\n","percentile75 = data_df['Annual_Premium'].quantile(0.75)\n","iqr = percentile75 - percentile25\n","upper_limit = percentile75 + 1.5 * iqr\n","lower_limit = percentile25 - 1.5 * iqr"],"metadata":{"id":"y7i-8n-5qLTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Capping\n","# Capping the data above the upper limit to upper limit & below the lower limit to the lower limit\n","\n","data_df['Annual_Premium'] = np.where(\n","    data_df['Annual_Premium'] > upper_limit,\n","    upper_limit,\n","    np.where(\n","        data_df['Annual_Premium'] < lower_limit,\n","        lower_limit,\n","        data_df['Annual_Premium']\n","    )\n",")"],"metadata":{"id":"-IopTw_G4l1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the boxplot again to check for outliers\n","sns.boxplot(x=data_df['Annual_Premium'])"],"metadata":{"id":"WHM6jkg94zrU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["Answer - I have used 'Capping' method to treat outliers. As there are only 3,81,109 entries in my dataset, trimming the outliers would lead to data loss."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"markdown","source":["Answer - To enhance the accuracy of our model predictions, we employed the code \"df_new = pd.get_dummies(df_new, drop_first=False)\" to encode categorical columns and generate dummy variables. This process of creating dummy variables aids in capturing categorical information, thereby improving the predictive performance of the model."],"metadata":{"id":"yDr3P8wNDIvo"}},{"cell_type":"code","source":["# Encode your categorical columns\n","# Using Pandas get Dummies for Encoding categorical features\n","data_df=pd.get_dummies(data_df,drop_first=True,sparse=True)\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.head()"],"metadata":{"id":"vgcqFv5e5dnl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["I have used One hot encoding, and also dropping the first column of each encoded column. This method is an effective technique used to represent categorical variables as numerical values for a machine learning model."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","\n","data_df['Driving_License'].value_counts()"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since practically every customer has a driver's license, it is useless to insure anyone without one because it would be deterimental to the business. Hence, we wouldn't provide vehicle insurance to someone who didn't have a license to drive. As we can drop driving license column as they are not providing any valuable information."],"metadata":{"id":"MKZsNny36vcC"}},{"cell_type":"code","source":["\n","# Dropping the 'Driving_License_Yes' column\n","data_df.drop(columns=['Driving_License'],axis=1,inplace=True)"],"metadata":{"id":"-Qma3Xyl7YEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","data_df_new.columns"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Answer - We utilized the VIF (Variance Inflation Factor) to eliminate unnecessary features during the pre-processing stage of modeling, enhancing the model's effectiveness by reducing multicollinearity."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Answer - After encoding the categorical columns and evaluating the VIF (Variance Inflation Factor) scores, we chose all the features that had VIF scores below 10 for further analysis and modeling."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"markdown","source":["No data transformation is required for our dataset."],"metadata":{"id":"9_JUQdODGw2a"}},{"cell_type":"code","source":["# Transform Your data"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","\n","scaler=StandardScaler()\n","X_train=scaler.fit_transform(X_train)\n","X_test=scaler.transform(X_test)\n"],"metadata":{"id":"uYqXLN1WolfN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?\n","\n","Answer - I have used standard scaler. As the dataset had values ranging from -30 & 3000, a scaling method had to be used for optimal model performance."],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Answer - We did not employ dimensionality reduction techniques as the number of variables in our dataset was relatively small and aligned with the desired dimension."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","# Train test split our data\n","\n","X_train,X_test,y_train,y_test = train_test_split(x_new,y_new, test_size=0.2,random_state=2)"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["I have used 80-20 split ratio. This optimal ratio provides enough data for the model to train and test.\n","This is a common splitting ratio used in machine learning, where a larger proportion of the data is used for training to ensure the model has enough data to learn from. The smaller proportion of data allocated for testing is used to evaluate the model's performance on unseen data, which helps to assess how well the model is generalizing to new data."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","# Target variable countplot\n","sns.countplot(x=data_df['Response'], data=data_df, palette='mako')"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see from the above graph, the dataset is imbalanced."],"metadata":{"id":"94FnH5VT-YAl"}},{"cell_type":"code","source":["#Defining X and y variable\n","\n","X=data_df.drop(['Response'],axis=1)\n","y=data_df['Response']"],"metadata":{"id":"fqWnQzsRXTnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling imbalanced dataset using SMOTE method\n","\n","sm=SMOTE()\n","x_new, y_new = sm.fit_resample(X, y.ravel())\n","\n","print(\"Before Using SMOTE, counts of label '1': {}\".format(sum(y == 1)))\n","print(\"Before Using SMOTE, counts of label '0': {} \\n\".format(sum(y == 0)))\n","print(\"After Using SMOTE, counts of label '1': {}\".format(sum(y_new == 1)))\n","print(\"After Using SMOTE, counts of label '0': {} \\n\".format(sum(y_new == 0)))\n","print('\\n')"],"metadata":{"id":"k_0D2izB-mkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the balanced dataset\n","\n","plt.figure(figsize = (14,6))\n","plt.subplot(1,2,1)\n","sns.countplot(x=data_df['Response'], data=data_df, palette='husl')\n","plt.title('Before sampling',fontsize=20)\n","plt.subplot(1,2,2)\n","sns.countplot(x=y_new,palette='husl')\n","plt.title('After sampling',fontsize=20)\n","plt.show()"],"metadata":{"id":"xxp4HwDA-4KB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I have used SMOTE method to handle the class imbalance because it simply duplicates examples from the minority class in the training dataset prior to fitting a model. Although this balances the class distribution, it does not provide any additional information to the model."],"metadata":{"id":"iOhkRvnH_Pz7"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"markdown","source":["**Logistic Regression**"],"metadata":{"id":"yY01uVAYTTVX"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","lr = LogisticRegression()\n","\n","# Fit the Algorithm\n","lr.fit(X_train,y_train)\n","\n","# Predict on the model\n","y_pred_lr = lr.predict(X_test)\n","y_pred_proba_lr = lr.predict_proba(X_test)[:,1]"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Evaluation\n","RS_lr= recall_score(y_test, y_pred_lr)\n","print(\"Recall_Score : \", RS_lr)\n","\n","PS_lr= precision_score(y_test, y_pred_lr)\n","print(\"Precision_Score :\",PS_lr)\n","\n","f1S_lr= f1_score(y_test, y_pred_lr)\n","print(\"f1_Score :\", f1S_lr)\n","\n","AS_lr= accuracy_score(y_test , y_pred_lr)\n","print(\"Accuracy_Score :\",AS_lr)\n","\n","acu_lr = roc_auc_score(y_test , y_pred_lr)\n","print(\"ROC_AUC Score:\",acu_lr)"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Confusion matrix\n","\n","cm_logistic = metrics.confusion_matrix(y_test, y_pred_lr)\n","print(cm_logistic)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_logistic, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"T29mVnNBA5e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_lr)\n","plt.title('Logistic Regression ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"HFyoDc_hBMxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ML model used is a Logistic Regression model. The classification report shows the precision, recall, and F1-score for each class, as well as the support (number of instances) for each class in the training set.\n","\n","The precision is the ratio of true positive predictions to the total number of positive predictions. The recall is the ratio of true positive predictions to the total number of actual positive instances in the dataset. The F1-score is the harmonic mean of precision and recall.\n","\n","Looking at the evaluation metric scores, we can see that the model has an overall accuracy of 81%, meaning that it correctly classified 81% of the instances in the training set. The precision for class 0 is 76%, meaning that when the model predicted a class 0 instance, it was correct 76% of the time. The recall for class 0 is 89%, meaning that the model correctly identified 89% of the actual class 0 instances in the dataset. The F1-score for class 0 is 82%.\n","\n","Overall, the model seems to be performing reasonably well, with an accuracy of 81% on test set."],"metadata":{"id":"xQJa-_QLBWgi"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n","grid_lr = GridSearchCV(lr, param_grid, cv=5)\n","\n","# Fit the Algorithm\n","grid_lr.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_gcv = grid_lr.predict(X_test)"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"code","source":["# Evaluation\n","RS_lr_gcv= recall_score(y_test, y_pred_gcv)\n","print(\"Recall_Score : \", RS_lr_gcv)\n","\n","PS_lr_gcv= precision_score(y_test, y_pred_gcv)\n","print(\"Precision_Score :\",PS_lr_gcv)\n","\n","f1S_lr_gcv = f1_score(y_test, y_pred_gcv)\n","print(\"f1_Score :\", f1S_lr_gcv)\n","\n","AS_lr_gcv = accuracy_score(y_test , y_pred_gcv)\n","print(\"Accuracy_Score :\",AS_lr_gcv)\n","\n","acu_lr_gcv = roc_auc_score(y_test , y_pred_gcv)\n","print(\"ROC_AUC Score:\",acu_lr_gcv)\n"],"metadata":{"id":"e2_cqZJ_CXS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best cross-validation score:\", grid_lr.best_score_)\n","print(\"Best parameters:\", grid_lr.best_params_)"],"metadata":{"id":"LTeupkR7ChAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best cross-validation score achieved was 0.81, and the best hyperparameter value for C was found to be 10.\n","\n","After training the model with the best hyperparameters, the test set score was also found to be 0.81. This suggests that the model is performing consistently well on both the training and test sets, and that it is unlikely to be overfitting.\n","\n","Overall, it appears that the logistic regression model with the selected hyperparameters is a good fit for the dataset, achieving an f1 score of 0.82."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2 Random Forest Classifier"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["# ML Model - 2 Implementation\n","rfc = RandomForestClassifier()\n","\n","# Fit the Algorithm\n","rfc.fit(X_train,y_train)\n","\n","# Predict on the model\n","y_pred_rfc = rfc.predict(X_test)\n","y_pred_proba_rfc = rfc.predict_proba(X_test)[:,1]"],"metadata":{"id":"_KNruoR5DAXp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Evaluation\n","RS_rfc= recall_score(y_test, y_pred_rfc)\n","print(\"Recall_Score : \", RS_rfc)\n","\n","PS_rfc= precision_score(y_test, y_pred_rfc)\n","print(\"Precision_Score :\",PS_rfc)\n","\n","f1S_rfc = f1_score(y_test, y_pred_rfc)\n","print(\"f1_Score :\", f1S_rfc)\n","\n","AS_rfc = accuracy_score(y_test , y_pred_rfc)\n","print(\"Accuracy_Score :\",AS_rfc)\n","\n","acu_rfc = roc_auc_score(y_test , y_pred_rfc)\n","print(\"ROC_AUC Score:\",acu_rfc)"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Confusion Matrix\n","\n","cm_rf= metrics.confusion_matrix(y_test, y_pred_rfc)\n","print(cm_rf)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_rf, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"-EZuPQBuD0Ol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROC Curve\n","\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_rfc)\n","plt.title('Random Forest Classifier ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"rwJKqyQqEA0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ML model used is Random Forest for classifier. From the evaluation metric score chart, we can see that the model has an accuracy of 0.86, which means that 86% of the predictions made by the model are correct. The precision for class 0 is 0.83, which means that out of all the positive predictions made for class 0, 83% of them are actually correct. The recall for class 1 is 0.91, which means that out of all the actual positive instances of class 1, the model correctly identified 91% of them. The F1-score for class 2 is 0.87, which is the harmonic mean of precision and recall, and provides an overall measure of the model's accuracy for that class.\n","\n","In summary, the Random Forest model has better performance on this classification task compared to logistic regression classifier."],"metadata":{"id":"D9_hHdqXEKIP"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","clsr = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=30, max_features='log2',\n","                       max_leaf_nodes=40, max_samples=None,\n","                       min_impurity_decrease=0.0,\n","                       min_samples_leaf=1, min_samples_split=4,\n","                       min_weight_fraction_leaf=0.0, n_estimators=200,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)\n","# Fit the Algorithm\n","clsr.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_rfc_gcv = clsr.predict(X_test)"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set"],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"code","source":["# Evaluation\n","RS_rfc_gcv= recall_score(y_test, y_pred_rfc_gcv)\n","print(\"Recall_Score : \", RS_rfc_gcv)\n","\n","PS_rfc_gcv = precision_score(y_test, y_pred_rfc_gcv)\n","print(\"Precision_Score :\",PS_rfc_gcv)\n","\n","f1S_rfc_gcv = f1_score(y_test, y_pred_rfc_gcv)\n","print(\"f1_Score :\", f1S_rfc_gcv)\n","\n","AS_rfc_gcv = accuracy_score(y_test , y_pred_rfc_gcv)\n","print(\"Accuracy_Score :\",AS_rfc_gcv)\n","\n","acu_rfc_gcv = roc_auc_score(y_test , y_pred_rfc_gcv)\n","print(\"ROC_AUC Score:\",acu_rfc_gcv)\n"],"metadata":{"id":"-974HCU3FcVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["No, we did not see any improvement here."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3  XG-Boost"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","xgb = XGBClassifier()\n","\n","# Fit the Algorithm\n","xgb.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_xgb = xgb.predict(X_test)\n","y_pred_proba_xgb = xgb.predict_proba(X_test)[:,1]\n"],"metadata":{"id":"ggmmuxMZGJqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Evaluation\n","RS_xgb= recall_score(y_test, y_pred_xgb)\n","print(\"Recall_Score : \", RS_xgb)\n","\n","PS_xgb= precision_score(y_test, y_pred_xgb)\n","print(\"Precision_Score :\",PS_xgb)\n","\n","f1S_xgb = f1_score(y_test, y_pred_xgb)\n","print(\"f1_Score :\", f1S_xgb)\n","\n","AS_xgb = accuracy_score(y_test , y_pred_xgb)\n","print(\"Accuracy_Score :\",AS_xgb)\n","\n","acu_xgb = roc_auc_score(y_test , y_pred_xgb)\n","print(\"ROC_AUC Score:\",acu_xgb)"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Visualizing evaluation Metric Score chart\n","# Confusion Matrix\n","\n","cm_rf= metrics.confusion_matrix(y_test, y_pred_xgb)\n","print(cm_rf)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_rf, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"iNmgKx-MGtCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb)\n","plt.title('Random Forest Classifier ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"UHVfDmC8GzSW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","params = {\n","    'max_depth': [3, 5],\n","    'learning_rate': [0.1],\n","    'n_estimators': [10, 20],\n","}\n","\n","# Perform cross-validation and hyperparameter tuning\n","grid_search = GridSearchCV(xgb, params, cv=5, scoring='accuracy')\n","\n","# Fit the Algorithm\n","grid_search.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_xgb_gcv = grid_search.predict(X_test)"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"code","source":["\n","# Evaluation\n","RS_xgb_gcv= recall_score(y_test, y_pred_xgb_gcv)\n","print(\"Recall_Score : \", RS_xgb_gcv)\n","\n","PS_xgb_gcv = precision_score(y_test, y_pred_xgb_gcv)\n","print(\"Precision_Score :\",PS_xgb_gcv)\n","\n","f1S_xgb_gcv = f1_score(y_test, y_pred_xgb_gcv)\n","print(\"f1_Score :\", f1S_xgb_gcv)\n","\n","AS_xgb_gcv = accuracy_score(y_test , y_pred_xgb_gcv)\n","print(\"Accuracy_Score :\",AS_xgb_gcv)\n","\n","acu_xgb_gcv = roc_auc_score(y_test , y_pred_xgb_gcv)\n","print(\"ROC_AUC Score:\",acu_xgb_gcv)\n"],"metadata":{"id":"xUtgJxL5IJty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["I have chosen f1-score score as it is best able to explain the fit of the data by taking the harmonic mean of precision score & recall score. F1-score is also the unweighted mean of these scores across all classes. In the best case, the macro average for precision, recall, and F1-score is 87%."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["I have chosen Random Forest Regressor as my final classifier model. With a f1-score score of 90%, we can consider random forest classifier as our best model."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"code","source":["features = x_new.columns\n","importances = rfc.feature_importances_\n","indices = np.argsort(importances)\n","\n","\n","#Plotting figure\n","plt.figure(figsize=(6,6))\n","plt.style.use('dark_background')\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"LZUWzYf-KzBC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see from the feature importance graph, the feature 'Previously_Insured_yes' can be considered as most important with relative importance of 0.2. The next 4 features are vintage, annual_premium, age and vehicle_damage-yes can be considered with relative importance ranging from 0.125-0.175. As these 5 main features play a role in decreasing the value of entropy, the machine learning model, random forest classifier considers them closer to the root node."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In our analysis, we initially performed EDA on all the features of our datset. We first analysed our dependent variable i.e, 'Response' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class. we also analysed numerical variable, check out the correlation, distribution and their relationship with the dependent variable. We then later hot encoded the categorical variables.\n","\n","Next we implemented 3 machine learning algorithms Logistic Regression, Random Forest Classifier, XG-Boost. We did some hyperparameter tuning to improve our model performance.\n","\n","Out of all above models Random forest Classifier gives the highest F1-score of 87% for test Set. No overfitting is seen.\n","\n","So finally, the insurance company can deploy a machine learning model that uses Random Forest Classifier to predict the wheather the already existing health insurance customer would be interested in a vehicle insurance product. The company can improve the conversion rate by taking steps to encourage people to buy vehicle insurance by offering some incentives/ease of application & claim settlement process. Cross selling might be an effective way to increase the profits since the customer acquisition cost still remains 0."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}},{"cell_type":"code","source":[],"metadata":{"id":"xxS8Qcuqs8p1"},"execution_count":null,"outputs":[]}]}